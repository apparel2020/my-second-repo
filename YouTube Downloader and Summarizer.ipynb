{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORJESC+YWOQqkp6aNwj59X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apparel2020/my-second-repo/blob/main/YouTube%20Downloader%20and%20Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube Audio Transcription and Summarization using Open Source Models\n",
        "# For Google Colab Free Tier - With YouTube Bot Detection Fix\n",
        "\n",
        "# Install required packages\n",
        "!pip install yt-dlp transformers sentencepiece datasets accelerate torch bitsandbytes peft optimum\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import subprocess\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import whisper\n",
        "\n",
        "# Function to download YouTube audio using yt-dlp (more reliable than pytubefix)\n",
        "def youtube_audio_downloader(link):\n",
        "    if not link or ('youtube.com' not in link and 'youtu.be' not in link):\n",
        "        print('Invalid YouTube link!')\n",
        "        return False\n",
        "\n",
        "    print('Downloading the audio stream...')\n",
        "\n",
        "    # Create output filename based on current timestamp\n",
        "    import time\n",
        "    output_filename = f\"audio_{int(time.time())}.mp3\"\n",
        "\n",
        "    # Use yt-dlp which has better anti-bot-detection capabilities\n",
        "    command = [\n",
        "        'yt-dlp',\n",
        "        '-x',  # Extract audio\n",
        "        '--audio-format', 'mp3',  # Convert to mp3\n",
        "        '--audio-quality', '0',  # Best quality\n",
        "        '-o', output_filename,  # Output filename\n",
        "        link  # YouTube URL\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        subprocess.run(command, check=True)\n",
        "        if os.path.exists(output_filename):\n",
        "            print('Download completed successfully!')\n",
        "            return output_filename\n",
        "        else:\n",
        "            print('Error: Download completed but file not found!')\n",
        "            return False\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f'Error downloading the file: {e}')\n",
        "        # Try alternate method if first method fails\n",
        "        try:\n",
        "            print('Trying alternate download method...')\n",
        "            alt_command = [\n",
        "                'yt-dlp',\n",
        "                '-f', 'bestaudio',  # Best audio format available\n",
        "                '--extract-audio',\n",
        "                '--audio-format', 'mp3',\n",
        "                '--audio-quality', '0',\n",
        "                '-o', output_filename,\n",
        "                '--no-check-certificates',  # Skip HTTPS certificate validation\n",
        "                '--user-agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36',\n",
        "                link\n",
        "            ]\n",
        "            subprocess.run(alt_command, check=True)\n",
        "            if os.path.exists(output_filename):\n",
        "                print('Alternate download method succeeded!')\n",
        "                return output_filename\n",
        "            else:\n",
        "                print('Error: Alternate download completed but file not found!')\n",
        "                return False\n",
        "        except subprocess.CalledProcessError as e2:\n",
        "            print(f'Error with alternate download method: {e2}')\n",
        "            return False\n",
        "\n",
        "# Function to transcribe audio using Whisper (open source version)\n",
        "def transcribe(audio_file, not_english=False):\n",
        "    if not os.path.exists(audio_file):\n",
        "        print('Audio file does not exist!')\n",
        "        return False\n",
        "\n",
        "    print('Loading Whisper model...')\n",
        "    # Use a smaller model to fit within Colab's free tier memory constraints\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    print('Starting transcription...')\n",
        "    if not_english:\n",
        "        # Translate to English\n",
        "        result = model.transcribe(audio_file, task=\"translate\")\n",
        "    else:\n",
        "        # Just transcribe\n",
        "        result = model.transcribe(audio_file)\n",
        "    print('Transcription completed!')\n",
        "\n",
        "    name, extension = os.path.splitext(audio_file)\n",
        "    transcript_filename = f'transcript-{name}.txt'\n",
        "    with open(transcript_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(result[\"text\"])\n",
        "\n",
        "    print(f'Transcript saved to {transcript_filename}')\n",
        "    return transcript_filename\n",
        "\n",
        "# Function to summarize text using an open source LLM (T5 version)\n",
        "def summarize(transcript_filename):\n",
        "    if not os.path.exists(transcript_filename):\n",
        "        print('The transcript file does not exist!')\n",
        "        return False\n",
        "\n",
        "    with open(transcript_filename, 'r', encoding='utf-8') as f:\n",
        "        transcript = f.read()\n",
        "\n",
        "    print('Loading summarization model (FLAN-T5)...')\n",
        "    # Use FLAN-T5 for summarization - efficient and works well on Colab's free tier\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    print(f\"Using device: {'CUDA' if device == 0 else 'CPU'}\")\n",
        "\n",
        "    summarizer = pipeline(\n",
        "        \"summarization\",\n",
        "        model=\"google/flan-t5-base\",\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Handle long transcripts by splitting into chunks\n",
        "    max_input_length = 500  # T5 has limited context window\n",
        "\n",
        "    if len(transcript) <= max_input_length:\n",
        "        chunks = [transcript]\n",
        "    else:\n",
        "        # Split by sentences to preserve meaning\n",
        "        import re\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', transcript)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) < max_input_length:\n",
        "                current_chunk += sentence + \" \"\n",
        "            else:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence + \" \"\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "    print(f'Processing transcript in {len(chunks)} chunks...')\n",
        "\n",
        "    # Process each chunk\n",
        "    summary_chunks = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f'Summarizing chunk {i+1}/{len(chunks)}...')\n",
        "\n",
        "        # Add summarization prompt\n",
        "        prompt = f\"\"\"Summarize this text: {chunk}\"\"\"\n",
        "\n",
        "        summary_part = summarizer(prompt, max_length=150, min_length=30)\n",
        "        summary_chunks.append(summary_part[0]['summary_text'])\n",
        "\n",
        "    # Combine all summaries\n",
        "    if len(summary_chunks) > 1:\n",
        "        print('Generating final summary from all chunks...')\n",
        "        combined_summary = \" \".join(summary_chunks)\n",
        "        final_prompt = f\"\"\"Create a coherent summary with a title, introduction,\n",
        "        key points as bullet points, and a conclusion from this text: {combined_summary}\"\"\"\n",
        "\n",
        "        final_summary = summarizer(final_prompt, max_length=300, min_length=100)[0]['summary_text']\n",
        "    else:\n",
        "        final_summary = summary_chunks[0]\n",
        "\n",
        "    print('Summarization completed!')\n",
        "    return final_summary\n",
        "\n",
        "# Alternative summarization function using a different open source model\n",
        "def summarize_with_llama(transcript_filename):\n",
        "    \"\"\"Use a Llama-based model for summarization. This is more powerful but may require\n",
        "    more resources than the T5 model.\"\"\"\n",
        "\n",
        "    if not os.path.exists(transcript_filename):\n",
        "        print('The transcript file does not exist!')\n",
        "        return False\n",
        "\n",
        "    print('Loading TinyLlama for summarization...')\n",
        "    # Use TinyLlama which is smaller and can run on Colab's free tier\n",
        "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "    # Load in 4-bit to save memory\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    with open(transcript_filename, 'r', encoding='utf-8') as f:\n",
        "        transcript = f.read()\n",
        "\n",
        "    # Calculate available context window\n",
        "    max_input_length = 2048  # Define maximum context length\n",
        "\n",
        "    # Handle long transcripts\n",
        "    if len(transcript) > max_input_length - 500:  # Reserve 500 tokens for the prompt\n",
        "        print(f\"Transcript too long ({len(transcript)} chars), truncating to fit context window\")\n",
        "        transcript = transcript[:max_input_length - 500]\n",
        "\n",
        "    # Create a prompt with instructions\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are a helpful AI assistant that creates concise summaries.\n",
        "<|user|>\n",
        "Create a summary of the following text.\n",
        "Text: {transcript}\n",
        "\n",
        "Add a title to the summary.\n",
        "Your summary should be informative and factual, covering the most important aspects of the topic.\n",
        "Start your summary with an INTRODUCTION PARAGRAPH that gives an overview of the topic FOLLOWED by BULLET POINTS if possible AND end the summary with a CONCLUSION PHRASE.\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "    print('Generating summary...')\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant's response\n",
        "    summary = summary.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    print('Summary generation completed!')\n",
        "    return summary\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*50)\n",
        "    print(\"YouTube Transcription and Summarization Tool\")\n",
        "    print(\"(Using Open Source Models on Google Colab Free Tier)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Ask for YouTube link\n",
        "    link = input('Enter the YouTube video URL: ')\n",
        "\n",
        "    # Download audio\n",
        "    mp3_file = youtube_audio_downloader(link)\n",
        "    if not mp3_file:\n",
        "        print(\"Failed to download audio. Please try a different video or check the URL.\")\n",
        "        exit()\n",
        "\n",
        "    # Ask if the content is not in English\n",
        "    not_english_input = input('Is the content not in English? (y/n): ').lower()\n",
        "    not_english = not_english_input.startswith('y')\n",
        "\n",
        "    # Transcribe\n",
        "    transcript_file = transcribe(mp3_file, not_english=not_english)\n",
        "    if not transcript_file:\n",
        "        print(\"Failed to transcribe audio.\")\n",
        "        exit()\n",
        "\n",
        "    # Choose summarization method based on available resources\n",
        "    print(\"\\nChoose summarization method:\")\n",
        "    print(\"1. FLAN-T5 (faster, less RAM usage)\")\n",
        "    print(\"2. TinyLlama (better quality, more RAM required)\")\n",
        "\n",
        "    model_choice = input('Enter your choice (1 or 2): ')\n",
        "\n",
        "    try:\n",
        "        if model_choice == '2':\n",
        "            summary = summarize_with_llama(transcript_file)\n",
        "        else:\n",
        "            summary = summarize(transcript_file)\n",
        "\n",
        "        # Save summary to file\n",
        "        summary_file = f\"summary_{os.path.basename(transcript_file)}\"\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(summary)\n",
        "\n",
        "        print('\\n\\nSUMMARY:')\n",
        "        print('='*50)\n",
        "        print(summary)\n",
        "        print('='*50)\n",
        "        print(f\"\\nSummary saved to {summary_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during summarization: {e}\")\n",
        "        print(\"If you selected TinyLlama and encountered an error, try FLAN-T5 instead (it requires less RAM).\")"
      ],
      "metadata": {
        "id": "A3TedpaoBSOi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}